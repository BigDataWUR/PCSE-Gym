{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mxUwCzU48qa"
      },
      "source": [
        "# Advanced Machine Learning course: CropGym tutorial\n",
        "## Nitrogen optimization on the winter wheat crop\n",
        "\n",
        "CropGym is a WUR-made reinforcement learning platform that trains an agent to optimize the usage of Nitrogen fertilizer on crops. The environment is a crop growth model (also developed in WUR), called LINTUL-3, capable of simulating a nitrogen-limited condition for the growth of plants.\n",
        "\n",
        "This notebook presents a use case for nitrogen application in winter wheat. An RL agent is trained to decide weekly on applying a discrete amount of nitrogen fertilizer, with the aim of achieving a balance between maximizing yield and minimizing environmental impact.\n",
        "\n",
        "This notebook is intended to be run on [google-colab](https://colab.research.google.com/). Trained models and derived results are available in the [results](https://github.com/BigDataWUR/PCSE-Gym/tree/master/notebooks/nitrogen-winterwheat/results) folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z30ea8E3tzfZ"
      },
      "source": [
        "### Install required packages and import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLYg3AjZ4-Jt"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!rm -fr /content/PCSE-Gym && cd /content/ && git clone https://github.com/BigDataWUR/PCSE-Gym.git\n",
        "!cd /content/PCSE-Gym\n",
        "\n",
        "!pip3 uninstall -y --quiet pcse\n",
        "!rm -fr /content/pcse && cd /content/ && git clone https://github.com/ajwdewit/pcse.git\n",
        "!cd /content/pcse && git reset --hard 7daa80a && git apply /content/PCSE-Gym/notebooks/nitrogen-winterwheat/pcse.patch &&  pip3 install --quiet -e .\n",
        "\n",
        "# hack to install stable-baselines3 v1.5.0 with gym v0.21.1\n",
        "!pip3 install git+https://github.com/Pythoniasm/gym-fork.git@fix-v0.21.0\n",
        "!git clone https://github.com/DLR-RM/stable-baselines3\n",
        "!cd /content/stable-baselines3 && git reset --hard v1.5.0\n",
        "# ugly hack to make sure that gym v0.21.1 is used, as well as torch 1.11.0 (the latter is needed for reproducibility)\n",
        "# colab will most likely complain about dependency conflicts, but that's OK for us\n",
        "!sed -i -e 's/0\\.21/0\\.21\\.1/g' -e 's/torch>=1\\.8\\.1/torch==1\\.11.0/g' /content/stable-baselines3/setup.py\n",
        "!cd /content/stable-baselines3 && pip3 install -e .\n",
        "\n",
        "import sys\n",
        "sys.path += ['/content/pcse','/content/stable-baselines3','/content/PCSE-Gym/','/content/PCSE-Gym/notebooks/nitrogen-winterwheat']\n",
        "\n",
        "# During development the following was used\n",
        "#  gym==0.21.0\n",
        "#  lib_programname==2.0.5\n",
        "#  matplotlib==3.3.4\n",
        "#  numpy==1.19.5\n",
        "#  pandas==1.3.0\n",
        "#  PyYAML==6.0\n",
        "#  scipy==1.7.0\n",
        "#  stable_baselines3==1.5.0\n",
        "#  torch==1.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DvnxWsOnuuj2"
      },
      "outputs": [],
      "source": [
        "# Some includes\n",
        "import os\n",
        "import gym\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set variables\n",
        "all_years = [*range(1990, 2022)]\n",
        "train_years = [year for year in all_years if year % 2 == 1]\n",
        "test_years = [year for year in all_years if year % 2 == 0]\n",
        "train_locations = [(52,5.5), (51.5,5), (52.5,6.0)]\n",
        "test_locations= [(52,5.5), (48,0)]\n",
        "location_to_label = {'52;5.5': 'NL', '48;0': 'FR'}\n",
        "colors = {'RL':'tab:blue', 'SP':'tab:orange', 'Ceres': 'red'}\n",
        "markers = {'52;5.5':'o', '48;0':'^'}\n",
        "random.seed(42)\n",
        "\n",
        "font = {'weight': 'bold', 'size': 14}\n",
        "ax = {'titleweight': 'bold', 'titlesize': 14}\n",
        "matplotlib.rc('font', **font)\n",
        "matplotlib.rc('axes', **ax)\n",
        "sns.set_theme(font_scale=1.00)\n",
        "\n",
        "inputdir='/content/PCSE-Gym/notebooks/nitrogen-winterwheat'\n",
        "resultsdir='/content/PCSE-Gym/notebooks/nitrogen-winterwheat/results'\n",
        "\n",
        "\n",
        "import itertools\n",
        "import functools as ft\n",
        "\n",
        "def read_data(resultsdir='/content/PCSE-Gym/notebooks/nitrogen-winterwheat/results', \n",
        "              csv_models={\"baseline\": \"fixed.csv\", \"upperbound\": \"upperbound.csv\", \n",
        "                          \"model\": [\"model-1.csv\", \"model-3.csv\", \"model-5.csv\", \"model-7.csv\", \"model-9.csv\", \n",
        "                                    \"model-15.csv\", \"model-19.csv\", \"model-42.csv\", \"model-47.csv\", \"model-70.csv\", \n",
        "                                    \"model-74.csv\", \"model-75.csv\", \"model-79.csv\", \"model-88.csv\", \"model-99.csv\"]}\n",
        "              ):\n",
        "  df_SP = pd.read_csv(os.path.join(resultsdir, csv_models[\"baseline\"]))\n",
        "  df_Ceres = pd.read_csv(os.path.join(resultsdir, csv_models[\"upperbound\"] ))\n",
        "  df_RL = [pd.read_csv(os.path.join(resultsdir, csv_model)) for csv_model in csv_models[\"model\"]]\n",
        "\n",
        "  dfs = list(itertools.chain([df_SP, df_Ceres, *df_RL]))\n",
        "  suffix = list(itertools.chain(['_SP', '_Ceres', *[f'_RL_{i}' for i in range(len(df_RL))]]))\n",
        "  duplicate_cols = ['TMIN', 'TMAX', 'IRRAD', 'RAIN', 'year', 'location']\n",
        "\n",
        "  for i, df in enumerate(dfs):\n",
        "    dfs[i].columns = [str(col) if col in duplicate_cols else str(col) + suffix[i] for col in df.columns]\n",
        "    if i>0:\n",
        "      dfs[i].drop(columns=duplicate_cols, inplace=True)\n",
        "  df_merged = ft.reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how = 'outer'), dfs)\n",
        "\n",
        "  #Convert to common units\n",
        "  df_merged[f'rain'] = 10.0*df_merged[f'RAIN']\n",
        "  filter_col = [col for col in df_merged if col.startswith('WSO')]\n",
        "  suffices = ([c.split('_', 1)[1] for c in filter_col])\n",
        "  for suffix in suffices:\n",
        "    df_merged[f'fertilizer_{suffix}'] = 10.0 * df_merged[f'fertilizer_{suffix}']\n",
        "    df_merged[f'WSO_{suffix}'] = 0.01 * df_merged[f'WSO_{suffix}']\n",
        "    df_merged[f'nitrogen_{suffix}'] = 10.0*(df_merged[f'fertilizer_{suffix}'])\n",
        "\n",
        "  return df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdQV3i2suXXr"
      },
      "source": [
        "## Getting to know the RL Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu7GzHVRh0XT"
      },
      "source": [
        "To set up a proper agent, one needs to be familiar with the environment of the RL agent. In this case, the environment is the LINTUL-3 crop growth model, made available through PCSE (Python Crop Simulation Environment)\n",
        "\n",
        "Using PCSE, we can look into different elements of the environment, and how a crop model is defined. In the following section we will play around a bit with the PCSE environment.\n",
        "\n",
        "There is no extensive knowledge of crop modelling required to train an RL agent for CropGym, but what's important is understanding the important parameters of the crop model and how the RL agent explores the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-YBsZctyPV-"
      },
      "outputs": [],
      "source": [
        "# some imports\n",
        "import os\n",
        "import pcse\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import yaml\n",
        "data_dir = r'/content/PCSE-Gym/pcse_gym/environment/configs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6_cIg9EQ2qX"
      },
      "source": [
        "In the cell below, we will define the files used to parameterize the crop model. There are 3 things to define for the model: the crop itself (winter wheat), the soil specifications (soil properties) and the site specifications (whether it's irrigated or not). This is all loaded and fed into the PCSE file reader function to then concatenate in the parameter provider function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bAZpjtonyvVu"
      },
      "outputs": [],
      "source": [
        "from pcse.fileinput import PCSEFileReader\n",
        "from pcse.base import ParameterProvider\n",
        "\n",
        "cropfile = os.path.join(data_dir, \"crop\", \"lintul3_winterwheat.crop\")\n",
        "soilfile = os.path.join(data_dir, \"soil\", \"lintul3_springwheat.soil\")\n",
        "sitefile = os.path.join(data_dir, \"site\", \"lintul3_springwheat.site\")\n",
        "\n",
        "crop = PCSEFileReader(cropfile)\n",
        "soil = PCSEFileReader(soilfile)\n",
        "site = PCSEFileReader(sitefile)\n",
        "\n",
        "parameterprovider = ParameterProvider(soildata=soil, cropdata=crop, sitedata=site)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GJhPP1GV7wH"
      },
      "source": [
        "Before initializing the model, we should obtain weather parameters. Here, we obtain the weather parameters from the power NASA database, specifically for weather in the Netherlands as determined by the lat and long arguments. We can see the range of data that is available with the resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJGxzFgc0N_O"
      },
      "outputs": [],
      "source": [
        "from pcse.db import NASAPowerWeatherDataProvider\n",
        "weatherdataprovider = NASAPowerWeatherDataProvider(latitude=52, longitude=5)\n",
        "print(weatherdataprovider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTQdYCa1Wob9"
      },
      "source": [
        "We should also obtain the agromanagement actions. The file we are about to load contains triggers for actions to do in the crop model. Try running the file and checking its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjpomFicBLBI"
      },
      "outputs": [],
      "source": [
        "from pcse.fileinput import YAMLAgroManagementReader\n",
        "agromanagement_file = os.path.join(\"/content/pcse/pcse/tests/test_data/lintul3_springwheat.agro\")\n",
        "agromanagement = YAMLAgroManagementReader(agromanagement_file)\n",
        "print(agromanagement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEdFWbzsYtdp"
      },
      "source": [
        "The file defines what steps to do in a certain time range. In the case above it does agormanagement for springwheat, starting from April and ending in October, with two actions of fertilization, one with 10 and the other with 5 gNm-2.\n",
        "\n",
        "Nevertheless, this file is to do pre-defined actions. Our RL agent in CropGym will not load such a file, but instead do many different trials of fertilizations to optimize the defined reward function.\n",
        "\n",
        "In the following cell, we will initialize a LINTUL-3 model based on the provided parameters above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-arB4y5GAu6y"
      },
      "outputs": [],
      "source": [
        "from pcse.models import LINTUL3\n",
        "lintul = LINTUL3(parameterprovider=parameterprovider,weatherdataprovider=weatherdataprovider,agromanagement=agromanagement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5GgXcinhSdaU"
      },
      "outputs": [],
      "source": [
        "#run the lintul model until it terminates. When it terminates is defined by the files provided.\n",
        "lintul.run_till_terminate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQwkXd5xS9Dv"
      },
      "outputs": [],
      "source": [
        "# Here we obtain the output of the models.\n",
        "output = lintul.get_output()\n",
        "df = pd.DataFrame(output).set_index(\"day\")\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXCiKqW4cIRf"
      },
      "source": [
        "The cell above shows the final few days of the simulation before the end of the simulation. There are 22 variables that are available to monitor. The interesting variables for us are:\n",
        "* TNSOIL: The available amount of N available in soil to take\n",
        "* WSO: The weight of the storage organs (grains)\n",
        "* LAI: The leaf area index\n",
        "* NUPPT: The total uptake of N over time; how well the crop is absorbing N from the soil.\n",
        "\n",
        "We can try making a graph of these variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXdZS-XOTN4B"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16,16))\n",
        "#for key, axis in zip(df.columns, axes.flatten()):\n",
        "df['TNSOIL'].plot(ax=axes[0][0], title='TNSOIL')\n",
        "df['NUPTT'].plot(ax=axes[0][1], title='NUPTT')\n",
        "df['LAI'].plot(ax=axes[1][0], title='LAI')\n",
        "df['WSO'].plot(ax=axes[1][1], title='WSO')\n",
        "\n",
        "\n",
        "fig.autofmt_xdate()\n",
        "fig.savefig(os.path.join(data_dir, \"lintul3_winterwheat.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNIY5uZ_e-fO"
      },
      "source": [
        "#### TO DO 1:\n",
        "* Look at the graph of TNSOIL. What causes the spikes in the N availability?\n",
        "* Look at NUPTT. Why does the N uptake seem to crawl at a certain point?\n",
        "* Look at LAI. What do you think causes the LAI to degrade slightly once then degrade exponentially after a month?\n",
        "* Look at WSO. What is approximately the end result of the yield? (in gN/m2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPHAWqbPiudV"
      },
      "source": [
        "## The RL agent in CropGym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPAOFzFfjTav"
      },
      "source": [
        "To get familiar with the CropGym setting, we will visualize the results of the paper from Kallenberg et al., 2023 (under review for publication), an extended version of the original CropGym paper from [Overweg et al, 2021](https://arxiv.org/abs/2104.04326). \n",
        "\n",
        "In CropGym, there are several RL spaces that was defined: the action space, the observation space and the reward function.\n",
        "\n",
        "The reward function is defined as follows:\n",
        "\\begin{align}\\mathbb{R_t} = (\\mathit{WSO}^{\\pi}_{t} - \\mathit{WSO}^{\\pi}_{t-1}) - (\\mathit{WSO}^{0}_{t} - \\mathit{WSO}^{0}_{t-1}) - \\beta N_{t}\\end{align}\n",
        "\n",
        "with a timestep t, the agent tries to maximize the difference of the weight of the storage organ of a fertilizing policy 𝐖𝐒𝐎ₜ when compared to a policy of not fertilizing at all 𝐖𝐒𝐎ₜᴼ (zero nitrogen policy). The agent is punished by the amount of fertilizer 𝐍ₜ used with a multiplier for the cost β. This cost is a multiplier that mimics the economic cost of nitrogen fertilzer. By default, the value of β is 10.\n",
        "\n",
        "The timestep is weekly, so one timestep equals 7 days, with an action taken on the 7th day.\n",
        "\n",
        "The action space is a discrete space of three possible fertilization levels: {0, 2, 4} gN/m2.\n",
        "\n",
        "The observation space consists of crop parameters that the RL agent can observe; in this case it is 9 variables from the crop and 3 variables from the weather.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifv023sGkvER"
      },
      "source": [
        "In the following cell, a graph will be produced that shows graphs of the nitrogen application and the reward obtained by the trained RL agent. The agent was trained with the PPO algorithm for a subset of years in the range of 1990 and 2022. The it was trained in the Netherlands and tested in France."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeqsSvuY84uZ"
      },
      "outputs": [],
      "source": [
        "# Policy visualization\n",
        "import pickle\n",
        "from helper import plot_variable, get_ylim_dict\n",
        "\n",
        "# Load paper results from disk\n",
        "# A few cells below you can generate your own results with one of the available models, or one of the baselines\n",
        "with open(os.path.join(resultsdir,'results_RL.pickle'), 'rb') as f:\n",
        "  plot_results = pickle.load(f)\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "sns.set_theme(font_scale=1.00)\n",
        "\n",
        "all_variables = list(list(plot_results.values())[0][0].keys())\n",
        "all_years = list(set([k[0] for k in list(plot_results.keys())]))\n",
        "all_locations = list(set([k[1] for k in list(plot_results.keys())]))\n",
        "print(f'Available variables: {all_variables}')\n",
        "print(f'Available years: {all_years}')\n",
        "print(f'Available locations: {all_locations}')\n",
        "\n",
        "# Specify plotting variables here (alternative suggestions are provided as commented-out code)\n",
        "plot_years = test_years #[1992, 2020]\n",
        "subset_keys = [(year, (52,5.5)) for year in plot_years]\n",
        "results_subset = {f'{subset_key}': plot_results[subset_key] for subset_key in subset_keys}\n",
        "plot_variables =  ['fertilizer', 'reward'] #, 'action', 'fertilizer','reward']\n",
        "\n",
        "plot_average = True #False\n",
        "if (len(plot_years)) > 5 and not plot_average: print(f'Warning: plotting {plot_years} at once; consider plot_average=True')\n",
        "figsize = (5*len(plot_variables), 5*len(plot_variables))\n",
        "fig, axes = plt.subplots(len(plot_variables), 1, sharex=True, figsize=figsize)  \n",
        "for i, variable in enumerate(plot_variables):\n",
        "  ax = axes if len(plot_variables) == 1 else axes[i]\n",
        "  plot_variable(results_subset, variable=variable, cumulative_variables = ['fertilizer', 'reward', 'IRRAD','RAIN'], ax=ax, ylim=get_ylim_dict()[variable], plot_average=plot_average, put_legend=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-NNAVIBpsF9"
      },
      "source": [
        "#### TO DO 2:\n",
        "* In the fertilizer graph, what amount did the agent fertilize in total?\n",
        "* What is the average first amount that the agent fertilized in the start of the season? When is this?\n",
        "* In the reward graph, why does the graph start moving down, then start moving up again around June?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGb41Qg_uaWV"
      },
      "source": [
        "## Training an RL agent in CropGym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vot4iAXwy1CD"
      },
      "source": [
        "Here we will train an RL agent with the two previously learned RL algorithms: DQN and PPO. The actual training process of the the full cropgym takes almost a whole workday to complete; hence, this notebook provides training with a subset of the locations and years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uaBau81048h"
      },
      "source": [
        "### Training a DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EjKk9VL_0WvQ"
      },
      "outputs": [],
      "source": [
        "from wrapper import ReferenceEnv\n",
        "from helper import EvalCallback\n",
        "from stable_baselines3 import DQN, PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "import argparse\n",
        "\n",
        "\n",
        "# setting the seed\n",
        "seed = 1\n",
        "\n",
        "# number of steps for training\n",
        "nsteps = 10_000\n",
        "# setting the value for beta\n",
        "costs_nitrogen = 10\n",
        "\n",
        "rootdir = '/content/PCSE-Gym'\n",
        "log_dir = os.path.join(rootdir, 'notebooks', 'nitrogen-winterwheat', 'tensorboard_logs', 'CropGym-Tutorial')\n",
        "\n",
        "# setting the training and testing years for the PCSE simulations\n",
        "all_years = [*range(1990, 1993)]\n",
        "train_years = [year for year in all_years if year % 2 == 1]\n",
        "test_years = [year for year in all_years if year % 2 == 0]\n",
        "\n",
        "# locations of the weather train and test; 52,5.5 is the Netherlands, and 48,0 is France.\n",
        "train_locations = [(52,5.5)]\n",
        "test_locations = [(48,0)]\n",
        "\n",
        "# choosing the crop and weather variables that the RL agent can observe; the observation space\n",
        "crop_features = [\"DVS\", \"TGROWTH\", \"LAI\", \"NUPTT\", \"TRAN\", \"TNSOIL\", \"TRAIN\", \"TRANRF\", \"WSO\"]\n",
        "weather_features = [\"IRRAD\", \"TMIN\", \"RAIN\"]\n",
        "action_features = []  # alternative: \"cumulative_nitrogen\"\n",
        "tag = f'Seed-{seed}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3vRkGm8b7Xcu"
      },
      "outputs": [],
      "source": [
        "#Make the RL environment with a wrapper for PCSE, feeding in all the information defined in the previous cell\n",
        "env_pcse_train = ReferenceEnv(crop_features=crop_features, action_features=action_features,\n",
        "                                  weather_features=weather_features,\n",
        "                                  costs_nitrogen=costs_nitrogen, years=train_years, locations=train_locations,\n",
        "                                  action_space=gym.spaces.Discrete(3), action_multiplier=2.0)\n",
        "env_pcse_train = Monitor(env_pcse_train)\n",
        "env_pcse_train = VecNormalize(DummyVecEnv([lambda: env_pcse_train]), norm_obs=True, norm_reward=True,\n",
        "                              clip_obs=10000., clip_reward=50000., gamma=0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYDngC90EtQx"
      },
      "source": [
        "Below you can start training a DQN model for cropgym. It will take around 11 minutes before it finishes training for 10.000 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU-UyYvZ7TkJ"
      },
      "outputs": [],
      "source": [
        "# Start training the DQN model and initialize live tensorboard\n",
        "# Click on the reload button button on the top right to refresh the tensorboard\n",
        "\n",
        "# Tensorboard\n",
        "dqn_model_name = f'{tag}-Ncosts-{costs_nitrogen}-DQN-run'\n",
        "folder = dqn_model_name + \"_1\" #Change this based on the run number\n",
        "log_dir_tensorboard = os.path.join(log_dir, folder)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $log_dir_tensorboard\n",
        "\n",
        "model_dqn = DQN('MlpPolicy', env_pcse_train, gamma=0.99, seed=seed, verbose=0, tensorboard_log=log_dir)\n",
        "\n",
        "print(f'train for {nsteps} steps with costs_nitrogen={costs_nitrogen} (seed={seed})')\n",
        "model_dqn.learn(total_timesteps=nsteps, callback=EvalCallback(test_years=test_years, train_years=train_years,\n",
        "            train_locations=train_locations, test_locations=test_locations, eval_freq=nsteps),\n",
        "            tb_log_name=dqn_model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Y0w0_8GYBm"
      },
      "source": [
        "#### TO DO 3:\n",
        "* In the tensorboard; open images on the tab above and check the WSO growth. How far did the agent manage to make it grow?\n",
        "* How does the reward look like? Does it seem that there were enough training steps for the DQN agent?\n",
        "* What was the first action of the agent? **Hint: There are three discrete actions of the agent {0, 2, 4} gN/m2.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a PPO agent"
      ],
      "metadata": {
        "id": "3ui0h-DNIA8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing PPO hyper parameters used in the paper\n",
        "\n",
        "from wrapper import get_policy_kwargs\n",
        "\n",
        "hyperparams =   {'batch_size': 64, 'n_steps': 2048, 'learning_rate': 0.0003, 'ent_coef': 0.0, 'clip_range': 0.3,\n",
        "                    'n_epochs': 10, 'gae_lambda': 0.95, 'max_grad_norm': 0.5, 'vf_coef': 0.5\n",
        "                    }\n",
        "hyperparams['policy_kwargs'] = {}\n",
        "hyperparams['policy_kwargs'] = get_policy_kwargs(crop_features=crop_features, weather_features=weather_features,\n",
        "                                                  action_features=action_features)\n",
        "hyperparams['policy_kwargs']['net_arch'] = [dict(pi=[128,128], vf=[128,128])]\n",
        "hyperparams['policy_kwargs']['ortho_init'] = False"
      ],
      "metadata": {
        "id": "UUzWbNNUJw6o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make the RL environment with a wrapper for PCSE, feeding it with the same information\n",
        "env_pcse_train = ReferenceEnv(crop_features=crop_features, action_features=action_features,\n",
        "                                  weather_features=weather_features,\n",
        "                                  costs_nitrogen=costs_nitrogen, years=train_years, locations=train_locations,\n",
        "                                  action_space=gym.spaces.Discrete(3), action_multiplier=2.0)\n",
        "env_pcse_train = Monitor(env_pcse_train)\n",
        "env_pcse_train = VecNormalize(DummyVecEnv([lambda: env_pcse_train]), norm_obs=True, norm_reward=True,\n",
        "                              clip_obs=10., clip_reward=50., gamma=0.99)\n"
      ],
      "metadata": {
        "id": "-9Hqe9eFIWbQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TO DO 4:\n",
        "\n",
        "* Train a PPO agent for 20.000 steps in the cell below and pass the hyperparameters above into the RL agent! *Hint: the hyperparameters are passed when initializing the agent*\n",
        "\n",
        "If the tensorboard from the DQN is model is still running, it might be required to close the tensorboard beforehand."
      ],
      "metadata": {
        "id": "H0FttMB_OKqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_model_name = f'{tag}-Ncosts-{costs_nitrogen}-PPO-run'\n",
        "folder = ppo_model_name + \"_1\" #Change this based on the run number\n",
        "log_dir_tensorboard = os.path.join(log_dir, folder)\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir $log_dir_tensorboard\n",
        "\n",
        "# TODO 4 here\n",
        "nsteps ="
      ],
      "metadata": {
        "id": "4GzEfxiVOVs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TO DO 5:\n",
        "* How many times did the agent fertilize? *Hint: look at the action graph in tensorboard*\n",
        "* Look at the value function in the tensorboard. What does it represent?"
      ],
      "metadata": {
        "id": "dxHM6tEFR-Sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looking into the Deep RL network used by the RL agent"
      ],
      "metadata": {
        "id": "QVtOsq1BHR9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will briefly look into the deep neural networks used by the RL agents.\n",
        "We can print and see the structure of the neural networks that is used by CropGym."
      ],
      "metadata": {
        "id": "095KA-jTZTAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_dqn.policy)"
      ],
      "metadata": {
        "id": "3_qq-lv8GspI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TO DO 6:\n",
        "* What is the activation function used for the DQN model?\n",
        "* What is the dimension of the hidden layers of the DQN neural network?\n",
        "* What is the size of the output layer of the DQN neural network? Why is it that size?\n",
        "* print the network of the PPO model and describe it!"
      ],
      "metadata": {
        "id": "X07B38KiQz6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 6 here\n"
      ],
      "metadata": {
        "id": "YyD5J1ohRnUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have (briefly) trained an RL agent that still couldn't properly apply nitrogen. With more training time, it's possible to make the agent a bit more intelligent.\n",
        "\n",
        "#### TO DO 7:\n",
        "* Train an agent (PPO or DQN) with 200.000 steps!\n",
        "* Try to vary the inputs and report what you changed and see in the evaluation (call tensor board)."
      ],
      "metadata": {
        "id": "rijE6ithW9kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small hints for TO DO 7\n",
        "# setting the seed\n",
        "seed = 1\n",
        "\n",
        "# number of steps for training\n",
        "nsteps = 10_000\n",
        "# setting the value for beta\n",
        "costs_nitrogen = 10\n",
        "\n",
        "# setting the training and testing years for the PCSE simulations\n",
        "all_years = [*range(1990, 2022)]\n",
        "train_years = [year for year in all_years if year % 2 == 1]\n",
        "test_years = [year for year in all_years if year % 2 == 0]\n",
        "\n",
        "# locations of the weather train and test; 52,5.5 is the Netherlands, and 48,0 is France.\n",
        "train_locations = [(52,5.5), (51.5,5), (52.5,6.0)]\n",
        "test_locations = [(52,5.5), (48,0)]\n",
        "\n",
        "# choosing the crop and weather variables that the RL agent can observe; the observation space\n",
        "crop_features = [\"DVS\", \"TGROWTH\", \"LAI\", \"NUPTT\", \"TRAN\", \"TNSOIL\", \"TRAIN\", \"TRANRF\", \"WSO\"]\n",
        "weather_features = [\"IRRAD\", \"TMIN\", \"RAIN\"]\n",
        "action_features = []  # alternative: \"cumulative_nitrogen\"\n",
        "tag = f'Seed-{seed}'"
      ],
      "metadata": {
        "id": "tOjHvKNUekz6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rest of TO DO 7 here"
      ],
      "metadata": {
        "id": "dlkPIwHvfQgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional things about the paper\n",
        "\n",
        "Below you can visualize some further results from the paper. In the paper, we obtain the upper bound of the fertilizing policy (i.e., the best practice  amount of fertilizer needed for each year and location). This is a baseline we call \"Ceres\" the all-knowing (-fertilizer-application). Ceres can apply a continuous, as opposed to discrete, amount of nitrogen.\n",
        "\n",
        "The cell below will calculate the upper bound of the fertilizer for NL and FR, and then it can be visualized; comparing it with the standard practice and the trained RL agent."
      ],
      "metadata": {
        "id": "JTKxRIQm3J2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9-1WnlOGpVD"
      },
      "outputs": [],
      "source": [
        "# Statistics\n",
        "from helper import report_ci\n",
        "\n",
        "# Specify variable to compute statistics for\n",
        "var = 'WSO' #reward #fertilizer\n",
        "\n",
        "df_merged = read_data() #read data stored on disk\n",
        "df_merged = df_merged[df_merged.year % 2 == 0]\n",
        "\n",
        "random.seed(42)    \n",
        "n_boot=10000\n",
        "\n",
        "name_var_RL = [col for col in df_merged.columns if f'{var}_RL' in col]\n",
        "cols_var_RL = [df_merged.columns.get_loc(col) for col in name_var_RL]\n",
        "\n",
        "for i, location in enumerate (['52;5.5','48;0']):\n",
        "  df_boot = df_merged.loc[df_merged[\"location\"] == location]\n",
        "  boot_RL, boot_SP, boot_delta_RL_SP, boot_Ceres = [], [], [], []\n",
        "  n_observations=len(df_boot.index)\n",
        "\n",
        "  # bootstrapping\n",
        "  for b in range(n_boot):\n",
        "    obs = random.choices(range(n_observations), k=n_observations)\n",
        "    seed = random.choices(cols_var_RL, k=n_observations)\n",
        "    var_RL = df_boot.values[obs, seed]\n",
        "    var_Ceres = df_boot.values[obs,[df_boot.columns.get_loc(f'{var}_Ceres')]*n_observations]\n",
        "    boot_Ceres.append(np.median(var_Ceres))\n",
        "    var_SP = df_boot.values[obs,[df_boot.columns.get_loc(f'{var}_SP')]*n_observations]\n",
        "    delta_RL_SP = var_RL - var_SP\n",
        "    boot_RL.append(np.median(var_RL))\n",
        "    boot_SP.append(np.median(var_SP))\n",
        "    boot_delta_RL_SP.append(np.median(delta_RL_SP))\n",
        "    \n",
        "  median_Ceres = np.median(df_boot[f'{var}_Ceres'])\n",
        "  median_SP = np.median(df_boot[f'{var}_SP'])\n",
        "  median_RL = np.median(pd.concat([df_boot[col] for col in name_var_RL]).to_numpy())\n",
        "  median_diff = np.median(pd.concat([df_boot[col] - df_boot[f'{var}_SP'] for col in name_var_RL]).to_numpy())\n",
        "\n",
        "  print(f'**{location_to_label[location]}**')\n",
        "  print(f'median_Ceres: {median_Ceres:0.2f} {report_ci(boot_Ceres)}')\n",
        "  print(f'median_SP: {median_SP:0.2f} {report_ci(boot_SP)}[')\n",
        "  print(f'median_RL: {median_RL:0.2f} {report_ci(boot_RL)}]')\n",
        "  print(f'median_RL-SP: {median_diff:0.2f} {report_ci(boot_delta_RL_SP, True)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edGw6F8YaRY3"
      },
      "outputs": [],
      "source": [
        "# Scatter plots\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "from matplotlib.container import ErrorbarContainer\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.lines as mlines\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from helper import identity_line\n",
        "\n",
        "\n",
        "# specify which variables to plot\n",
        "markers = {'52;5.5':'o'} # alternatives e.g. {'52;5.5':'o', '48;0':'^'}\n",
        "plots = {0:('reward_Ceres', 'reward'), 1:('fertilizer_Ceres', 'fertilizer'), 2:('rain', 'fertilizer')} #2:('WSO_Ceres', 'WSO')}\n",
        "modes = ['Ceres', 'SP', 'RL']\n",
        "do_annotate = False\n",
        "\n",
        "df_merged = read_data()\n",
        "#only include test years (even years):\n",
        "df_merged = df_merged[df_merged.year % 2 == 0] \n",
        "\n",
        "sns.set_theme(font_scale=1.00)\n",
        "location=list(markers.keys())[0]\n",
        "line = mlines.Line2D([], [], marker=markers[location], color=colors['RL'], linestyle='None', markersize=6)\n",
        "capline = mlines.Line2D([], [], marker=markers[location], color=colors['RL'], linestyle='None', markersize=0)\n",
        "barline = LineCollection(np.empty((2,2,2)))\n",
        "RL= ErrorbarContainer((line, [capline], [barline]), has_xerr=False, has_yerr=True, label='RL (median+IQR)')\n",
        "\n",
        "fig, axes_scatter = plt.subplots(1, len(plots), figsize=(len(plots)*8,8), sharey='col', sharex='col')\n",
        "ax_scatter = axes_scatter if len(plots) == 1 else axes_scatter[0]\n",
        "\n",
        "legend_elements = [mlines.Line2D([], [], marker=markers[location], color=colors['Ceres'], linestyle='None', label='Ceres', markersize=6),\n",
        "                   mlines.Line2D([], [], marker=markers[location], color=colors['SP'], linestyle='None', label='Standard Practice', markersize=6),\n",
        "                   RL,\n",
        "                   mlines.Line2D([], [], marker=markers[location], color='black', linestyle='None', label='median', markersize=14, markerfacecolor=ax_scatter.get_facecolor())]\n",
        "\n",
        "for p, (x_orig, y_orig) in plots.items():\n",
        "  same_units = (x_orig.split('_', 1)[0] == y_orig.split('_', 1)[0])\n",
        "  do_regression = not same_units\n",
        "  ax_scatter = axes_scatter if len(plots) == 1 else axes_scatter[p]\n",
        "  title = f'{x_orig}/{y_orig}'\n",
        "  for i, m in enumerate(modes):\n",
        "    x = f'{x_orig}'\n",
        "    y = f'{y_orig}_RL_1' if m == 'RL' else f'{y_orig}_{m}'\n",
        "    r = df_merged[x].corr(df_merged[y])\n",
        "    title = title + f'\\n (r_{m}={r:0.2f})'\n",
        "    for i, location in enumerate (list(markers.keys())):\n",
        "      df_scatter = df_merged.loc[df_merged[\"location\"] == location]\n",
        "      r = df_scatter[x].corr(df_scatter[y])\n",
        "      title = title + f' (r_{location_to_label[location]}={r:0.2f}) (n={len(df_scatter[x])})'\n",
        "      if m == 'RL':\n",
        "        name_var_RL = [col for col in df_scatter.columns if f'{y_orig}_RL' in col]\n",
        "        y_median = np.median(df_scatter[name_var_RL], axis=1)\n",
        "        q25, q75 = np.percentile(df_scatter[name_var_RL], [25, 75], axis=1)\n",
        "        asymmetric_error = np.array(list(zip(y_median-q25, q75-y_median))).T\n",
        "        _, caps, bars = ax_scatter.errorbar(x=df_scatter[x], y=y_median, yerr=asymmetric_error, fmt='.', ecolor=colors[m], marker=markers[location], capsize=0)\n",
        "        [bar.set_alpha(0.5) for bar in bars]\n",
        "        [cap.set_alpha(0.5) for cap in caps]\n",
        "      else:\n",
        "        y_median = df_scatter[y].values\n",
        "        ax_scatter.scatter(df_scatter[x], df_scatter[y], s=30, color=colors[m], marker=markers[location])\n",
        "\n",
        "      if do_regression:\n",
        "        reg, = ax_scatter.plot([], [],color=colors[m], alpha=0.3)\n",
        "        clf = Ridge(alpha=0.5)\n",
        "        clf.fit(df_scatter[x].values.reshape(-1, 1), y_median.reshape(-1, 1))\n",
        "        f = clf.predict\n",
        "        reg.set_data([np.min(df_scatter[x]), np.max(df_scatter[x])], [f(np.min(df_scatter[x]).reshape(-1, 1)),f(np.max(df_scatter[x]).reshape(-1, 1))])    \n",
        "\n",
        "      if do_annotate:\n",
        "        df_scatter['y'] = y_median\n",
        "        for index, row in df_scatter.iterrows():\n",
        "          id = f\"{row['year']}\"\n",
        "          ax_scatter.annotate(id, (row[x], row['y']), fontsize=6, color=colors[m]).set_alpha(.7)\n",
        "\n",
        "      ax_scatter.scatter(np.median(df_scatter[x]), np.median(df_scatter[y]), s=150, edgecolors=colors[m], marker=markers[location], facecolors=ax_scatter.get_facecolor(), linewidths=3, zorder=3).set_alpha(.9)\n",
        "\n",
        "  if same_units:\n",
        "    identity_line(ax=ax_scatter, color=colors['Ceres'])\n",
        "    low_x, high_x = ax_scatter.get_xlim()\n",
        "    low_y, high_y = ax_scatter.get_ylim()\n",
        "    low = min(low_x, low_y)\n",
        "    high = max(high_x, high_y)\n",
        "    ax_scatter.set_xlim(low, high)\n",
        "    ax_scatter.set_ylim(low, high)\n",
        "  \n",
        "  if (x_orig.split('_', 1)[0] in ['fertilizer', 'reward']): ax_scatter.axvline(x=0, color='lightgrey', zorder=1)\n",
        "  if (y_orig.split('_', 1)[0] in ['fertilizer', 'reward']): ax_scatter.axhline(y=0, color='lightgrey', zorder=1)\n",
        "  ax_scatter.set_xlabel(f'{x}')\n",
        "  ax_scatter.set_ylabel(f'{y_orig}')\n",
        "  ax_scatter.set_title(f'{title}')\n",
        "  ax_scatter.yaxis.set_label_coords(-.1, 0.5)\n",
        "  ax_scatter.legend(handles=legend_elements, loc='upper left')\n",
        "\n",
        "  print(f'{resultsdir}/image-scatter-{y_orig}-{location_to_label[location]}.png')    \n",
        "  fig.savefig(f'{resultsdir}/image-scatter-{y_orig}-{location_to_label[location]}.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}